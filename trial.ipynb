{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import ffmpeg\n",
    "import torch\n",
    "import torchaudio\n",
    "import demucs\n",
    "from pydub import AudioSegment\n",
    "import subprocess\n",
    "import math\n",
    "\n",
    "from demucs import pretrained\n",
    "from demucs.apply import apply_model\n",
    "from mir_eval import separation\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB_PLUS\n",
    "from torchaudio.utils import download_asset\n",
    "from torchaudio.transforms import Fade\n",
    "\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Audio\n",
    "import warnings\n",
    "import json\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters\n",
    "SAMPLE_RATE = 44100\n",
    "DURATION = 30\n",
    "SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving MFCC's\n",
    "def extract_features(n_mfcc=13, n_fft=2048, hop_length=512, num_segments=5):\n",
    "    \"\"\"Take the input of the dataset and saves the JSON file to a particular folder containing MFCCs influenced by the parameters taken as input \n",
    "\n",
    "    Args:\n",
    "        dataset_path (_type_): Preprocessed daataset path to extract the MFCCs\n",
    "        json_path (_type_): JSON file path to save the MFCCs\n",
    "        n_mfcc (int, optional): Number of MFCC Coefficients. Defaults to 13.\n",
    "        n_fft (int, optional): Number of Fast Fourier Transform Filters. Defaults to 2048.\n",
    "        hop_length (int, optional): Number of Frames to skip after the previous one. Defaults to 512.\n",
    "        num_segments (int, optional): Number of segments, the audio file should split into. Defaults to 5.\n",
    "    \"\"\"\n",
    "    # Dictionary to store the data\n",
    "    data = {\n",
    "        \"mapping\": [],\n",
    "        \"mfcc\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "    \n",
    "    dataset_path = r\"E:\\Projects\\Song-Language-Classifier\\WhatsThatTongue\\database\\chunks\"\n",
    "    json_path = r\"E:\\Projects\\Song-Language-Classifier\\WhatsThatTongue\\database\\mfcc.json\"\n",
    "    \n",
    "    num_samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)\n",
    "    expected_num_mfcc_vectors_per_second = math.ceil(num_samples_per_segment / hop_length)\n",
    "    \n",
    "    print(num_samples_per_segment, expected_num_mfcc_vectors_per_second)\n",
    "    \n",
    "    # Loop through all audio files\n",
    "    for i, (dir_path, dir_names, file_names) in enumerate(os.walk(dataset_path)):\n",
    "        # print(i)\n",
    "        # print(dir_path, dir_names, file_names)\n",
    "        \n",
    "        # Process files for specific language\n",
    "        for f in file_names:\n",
    "            file_path = os.path.join(dir_path, f)\n",
    "            signal, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "            \n",
    "            print(f)\n",
    "            print(file_path)\n",
    "            print(num_segments)\n",
    "            \n",
    "            counter = 0\n",
    "            \n",
    "            # Process segments, extracting mfccs and storing data\n",
    "            for s in range(num_segments):\n",
    "                print(s)\n",
    "                start_sample = num_samples_per_segment * s\n",
    "                finish_sample = start_sample + num_samples_per_segment\n",
    "                \n",
    "                print(start_sample, finish_sample)\n",
    "                \n",
    "                print(signal[start_sample:finish_sample])\n",
    "                \n",
    "                mfcc = librosa.feature.mfcc(y=signal[start_sample:finish_sample],\n",
    "                                            sr=sr,\n",
    "                                            n_mfcc=n_mfcc,\n",
    "                                            n_fft=n_fft,\n",
    "                                            hop_length=hop_length\n",
    "                                            )\n",
    "                \n",
    "                # print(mfcc)\n",
    "                # print(mfcc)\n",
    "                \n",
    "                mfcc = mfcc.T\n",
    "                \n",
    "                # print(mfcc)\n",
    "                \n",
    "                print(len(mfcc))\n",
    "                print(expected_num_mfcc_vectors_per_second)\n",
    "                \n",
    "                # Store MFCC for segment if it has the expected length\n",
    "                if len(mfcc) == expected_num_mfcc_vectors_per_second:\n",
    "                    # print(mfcc)\n",
    "                    mfcc.tolist()\n",
    "                    data[\"mfcc\"].append(mfcc.tolist())\n",
    "                    # data[\"labels\"].append(i-1)\n",
    "                    print(f\"{file_path}, segment:{s}\")\n",
    "                \n",
    "                if counter == 0:\n",
    "                    break\n",
    "                    \n",
    "                counter += 1\n",
    "            break\n",
    "    \n",
    "    print(data)\n",
    "    \n",
    "    with open(json_path, \"w\") as fp:\n",
    "        json.dump(data, fp, indent=4)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264600 517\n",
      "chunk_000.wav\n",
      "E:\\Projects\\Song-Language-Classifier\\WhatsThatTongue\\database\\chunks\\chunk_000.wav\n",
      "5\n",
      "0\n",
      "0 264600\n",
      "[9.7656250e-04 9.4604492e-04 4.5776367e-05 ... 2.4414062e-04 2.4414062e-04\n",
      " 2.4414062e-04]\n",
      "32\n",
      "517\n",
      "{'mapping': [], 'mfcc': [], 'labels': []}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_features(n_mfcc=13, n_fft=2048, hop_length=512, num_segments=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
